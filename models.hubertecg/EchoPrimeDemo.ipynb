{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd78301",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = \"model_data/example_study\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adef3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Third-party library imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pydicom\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "# Local module imports\n",
    "import utils\n",
    "import video_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c561a",
   "metadata": {},
   "source": [
    "## Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cd8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc584b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_data/weights/echo_prime_encoder.pt\",map_location=device)\n",
    "echo_encoder = torchvision.models.video.mvit_v2_s()\n",
    "echo_encoder.head[-1] = torch.nn.Linear(echo_encoder.head[-1].in_features, 512)\n",
    "echo_encoder.load_state_dict(checkpoint)\n",
    "echo_encoder.eval()\n",
    "echo_encoder.to(device)\n",
    "for param in echo_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "vc_checkpoint = torch.load(\"model_data/weights/view_classifier.ckpt\",map_location=device)\n",
    "vc_state_dict={key[6:]:value for key,value in vc_checkpoint['state_dict'].items()}\n",
    "view_classifier = torchvision.models.convnext_base()\n",
    "view_classifier.classifier[-1] = torch.nn.Linear(\n",
    "    view_classifier.classifier[-1].in_features, 11\n",
    ")\n",
    "view_classifier.load_state_dict(vc_state_dict)\n",
    "view_classifier.to(device)\n",
    "view_classifier.eval()\n",
    "for param in view_classifier.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5a52c",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f540de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_take=32\n",
    "frame_stride=2\n",
    "video_size=224\n",
    "mean = torch.tensor([29.110628, 28.076836, 29.096405]).reshape(3, 1, 1, 1)\n",
    "std = torch.tensor([47.989223, 46.456997, 47.20083]).reshape(3, 1, 1, 1)\n",
    "\n",
    "def process_dicoms(INPUT):\n",
    "    \"\"\"\n",
    "    Reads DICOM video data from the specified folder and returns a tensor \n",
    "    formatted for input into the EchoPrime model.\n",
    "\n",
    "    Args:\n",
    "        INPUT (str): Path to the folder containing DICOM files.\n",
    "\n",
    "    Returns:\n",
    "        stack_of_videos (torch.Tensor): A float tensor of shape  (N, 3, 16, 224, 224)\n",
    "                                        representing the video data where N is the number of videos,\n",
    "                                        ready to be fed into EchoPrime.\n",
    "    \"\"\"\n",
    "\n",
    "    dicom_paths = glob.glob(f'{INPUT}/**/*.dcm',recursive=True)\n",
    "    stack_of_videos=[]\n",
    "    for idx, dicom_path in tqdm(enumerate(dicom_paths),total=len(dicom_paths)):\n",
    "        try:\n",
    "            # simple dicom_processing\n",
    "            dcm=pydicom.dcmread(dicom_path)\n",
    "            pixels = dcm.pixel_array\n",
    "            \n",
    "            # exclude images like (600,800) or (600,800,3)\n",
    "            if pixels.ndim < 3 or pixels.shape[2]==3:\n",
    "                continue \n",
    "                \n",
    "            # if single channel repeat to 3 channels    \n",
    "            if pixels.ndim==3:\n",
    "                \n",
    "                pixels = np.repeat(pixels[..., None], 3, axis=3)\n",
    "            \n",
    "            # mask everything outside ultrasound region\n",
    "            pixels=video_utils.mask_outside_ultrasound(dcm.pixel_array)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #model specific preprocessing\n",
    "            x = np.zeros((len(pixels),224,224,3))\n",
    "            for i in range(len(x)):\n",
    "                x[i] = video_utils.crop_and_scale(pixels[i])\n",
    "            \n",
    "            x = torch.as_tensor(x, dtype=torch.float).permute([3,0,1,2])\n",
    "            # normalize\n",
    "            x.sub_(mean).div_(std)\n",
    "        \n",
    "            ## if not enough frames add padding\n",
    "            if x.shape[1] < frames_to_take:\n",
    "                padding = torch.zeros(\n",
    "                (\n",
    "                    3,\n",
    "                    frames_to_take - x.shape[1],\n",
    "                    video_size,\n",
    "                    video_size,\n",
    "                ),\n",
    "                dtype=torch.float,\n",
    "                )\n",
    "                x = torch.cat((x, padding), dim=1)\n",
    "                \n",
    "            start=0\n",
    "            stack_of_videos.append(x[:, start : ( start + frames_to_take) : frame_stride, : , : ])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"corrupt file\")\n",
    "            print(str(e))\n",
    "\n",
    "    stack_of_videos=torch.stack(stack_of_videos)\n",
    "    \n",
    "    return stack_of_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49d9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_videos(stack_of_videos):\n",
    "    \"\"\"\n",
    "    Given a set of videos that belong to one echocardiogram study,\n",
    "    embed them in the latent space using EchoPrime encoder\n",
    "    \n",
    "    Args:\n",
    "        stack_of_videos (torch.Tensor): A float tensor of shape (N, 3, 16, 224, 224)\n",
    "                                        with preprocessed echo video data\n",
    "        \n",
    "    Returns:\n",
    "        stack_of_features (torch.Tensor) A float tensor of shape (N, 512)\n",
    "                                         with latent embeddings corresponding to echo videos\n",
    "    \"\"\"\n",
    "    bin_size=50\n",
    "    n_bins=math.ceil(stack_of_videos.shape[0]/bin_size)\n",
    "    stack_of_features_list=[]\n",
    "    with torch.no_grad():\n",
    "        for bin_idx in range(n_bins):\n",
    "            start_idx = bin_idx * bin_size\n",
    "            end_idx = min( (bin_idx + 1) * bin_size, stack_of_videos.shape[0])\n",
    "            bin_videos = stack_of_videos[start_idx:end_idx].to(device)\n",
    "            bin_features = echo_encoder(bin_videos)\n",
    "            stack_of_features_list.append(bin_features)\n",
    "        stack_of_features=torch.cat(stack_of_features_list,dim=0)\n",
    "    return stack_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19fe29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_views(stack_of_videos, visualize=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        stack_of_videos (torch.Tensor): A float tensor with preprocessed echo video data\n",
    "        \n",
    "    Returns:\n",
    "        stack_of_view_encodings (torch.Tensor) A float tensor of one hot embeddings with shape (N, 11)\n",
    "                                               representing echocardiogram views\n",
    "    \"\"\"\n",
    "    ## get views   \n",
    "    stack_of_first_frames = stack_of_videos[:,:,0,:,:].to(device)\n",
    "    with torch.no_grad():\n",
    "        out_logits=view_classifier(stack_of_first_frames)\n",
    "    out_views=torch.argmax(out_logits,dim=1)\n",
    "    view_list = [utils.COARSE_VIEWS[v] for v in out_views]\n",
    "    stack_of_view_encodings = torch.stack([torch.nn.functional.one_hot(out_views,11)]).squeeze().to(device)\n",
    "\n",
    "    # visualize images and the assigned views\n",
    "    if visualize:\n",
    "        print(\"Preprocessed and normalized video inputs\")\n",
    "        rows, cols = (len(view_list) // 12 + (len(view_list) % 9 > 0)), 12\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols, rows))\n",
    "        axes = axes.flatten()\n",
    "        for i in range(len(view_list)):\n",
    "            display_image = (stack_of_first_frames[i].cpu().permute([1,2,0]) * 255).numpy()\n",
    "            display_image = np.clip(display_image, 0, 255).astype('uint8')\n",
    "            display_image = np.ascontiguousarray(display_image)\n",
    "            display_image = cv2.cvtColor(display_image, cv2.COLOR_RGB2BGR)\n",
    "            cv2.putText(display_image, view_list[i].replace(\"_\",\" \"), (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 220, 255), 2)\n",
    "            axes[i].imshow(display_image)\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    return stack_of_view_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc3d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_study(INPUT,visualize=False):\n",
    "    \"\"\"\n",
    "    Produces an EchoPrime embedding of the echocardiography study\n",
    "\n",
    "    Args:\n",
    "        INPUT (str): Path to the folder containing DICOM files.\n",
    "        \n",
    "    Returns:\n",
    "        encoded_study (torch.Tensor): A float tensor of shape (N, 523)\n",
    "    \"\"\"\n",
    "    stack_of_videos=process_dicoms(INPUT)\n",
    "    stack_of_features=embed_videos(stack_of_videos)\n",
    "    stack_of_view_encodings=get_views(stack_of_videos,visualize)\n",
    "    encoded_study = torch.cat( (stack_of_features ,stack_of_view_encodings),dim=1)\n",
    "    \n",
    "    return encoded_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723395ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 50/50 [00:15<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_study=encode_study(INPUT, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6613c9e",
   "metadata": {},
   "source": [
    "# Obtain Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MIL weights per section\n",
    "MIL_weights = pd.read_csv(\"MIL_weights.csv\")\n",
    "non_empty_sections=MIL_weights['Section']\n",
    "section_weights=MIL_weights.iloc[:,1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec8ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1230676/1230676 [00:15<00:00, 79649.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load candidate reports\n",
    "candidate_studies=list(pd.read_csv(\"model_data/candidates_data/candidate_studies.csv\")['Study'])\n",
    "candidate_embeddings_p1=torch.load(\"model_data/candidates_data/candidate_embeddings_p1.pt\")\n",
    "candidate_embeddings_p2=torch.load(\"model_data/candidates_data/candidate_embeddings_p2.pt\")\n",
    "candidate_embeddings=torch.cat((candidate_embeddings_p1,candidate_embeddings_p2),dim=0)\n",
    "# Optionally normalize each candidate embedding vector\n",
    "# candidate_embeddings = torch.nn.functional.normalize(candidate_embeddings,dim=1)\n",
    "candidate_reports=pd.read_pickle(\"model_data/candidates_data/candidate_reports.pkl\")\n",
    "candidate_reports = [utils.phrase_decode(vec_phr) for vec_phr in tqdm(candidate_reports)]\n",
    "candidate_labels = pd.read_pickle(\"model_data/candidates_data/candidate_labels.pkl\")\n",
    "section_to_phenotypes = pd.read_pickle(\"section_to_phenotypes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13585355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(study_embedding: torch.Tensor) -> str:\n",
    "    \"\"\"\n",
    "    Given the EchoPrime study embedding generate a report\n",
    "    for each section focus on the views weighted\n",
    "    Args:\n",
    "        study_embedding - torch tensor of shape num_videos x 572\n",
    "        original_report - text for original study\n",
    "    \"\"\"\n",
    "    study_embedding=study_embedding.cpu()\n",
    "    generated_report=\"\"\n",
    "    for s_dx, sec in enumerate(non_empty_sections):\n",
    "        # need to multiply it based on what section does the view belong to.\n",
    "        cur_weights=[section_weights[s_dx][torch.where(ten==1)[0]] for ten in study_embedding[:,512:]]\n",
    "        no_view_study_embedding = study_embedding[:,:512] * torch.tensor(cur_weights,dtype=torch.float).unsqueeze(1)\n",
    "        # weights by views.\n",
    "        no_view_study_embedding=torch.mean(no_view_study_embedding,dim=0)\n",
    "        no_view_study_embedding=torch.nn.functional.normalize(no_view_study_embedding,dim=0)\n",
    "        similarities=no_view_study_embedding @ candidate_embeddings.T\n",
    "        \n",
    "        extracted_section=\"Section not found.\"\n",
    "        while extracted_section==\"Section not found.\":\n",
    "            max_id = torch.argmax(similarities)\n",
    "            predicted_section = candidate_reports[max_id]\n",
    "            extracted_section = utils.extract_section(predicted_section,sec)\n",
    "            if extracted_section != \"Section not found.\":\n",
    "                generated_report+= extracted_section\n",
    "            similarities[max_id]=float('-inf')\n",
    "            \n",
    "    return generated_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70cdd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_metrics(study_embedding: torch.Tensor,\n",
    "                   k=50) -> dict:\n",
    "    \"\"\"\n",
    "    study_embedding is a set of embeddings of all videos from the study e.g (52,512)\n",
    "    Takes a study embedding as input and\n",
    "    outputs a dictionary for a set of 26 features\n",
    "    \"\"\"\n",
    "    #per_section_study_embedding has shape (15,512)\n",
    "    per_section_study_embedding=torch.zeros(len(non_empty_sections),512)\n",
    "    study_embedding=study_embedding.cpu()\n",
    "    # make per section study embedding\n",
    "    for s_dx, sec in enumerate(non_empty_sections):\n",
    "        # get section weights\n",
    "        this_section_weights=[section_weights[s_dx][torch.where(view_encoding==1)[0]]\n",
    "                      for view_encoding in study_embedding[:,512:]]\n",
    "        this_section_study_embedding = (study_embedding[:,:512] * \\\n",
    "                                        torch.tensor(this_section_weights,\n",
    "                                                     dtype=torch.float).unsqueeze(1))\n",
    "        \n",
    "        #weighted average\n",
    "        this_section_study_embedding=torch.sum(this_section_study_embedding,dim=0)\n",
    "        per_section_study_embedding[s_dx]=this_section_study_embedding\n",
    "        \n",
    "    per_section_study_embedding=torch.nn.functional.normalize(per_section_study_embedding)\n",
    "    #similarities has shape (15,230676)\n",
    "    similarities=per_section_study_embedding @ candidate_embeddings.T\n",
    "\n",
    "    # for each row find indices of 50 highest values\n",
    "    #top_candidate_ids has shape (15,50)\n",
    "    top_candidate_ids=torch.topk(similarities, k=k, dim=1).indices\n",
    "    #now predict for each phenotype:\n",
    "    preds={}\n",
    "    for s_dx, section in enumerate(section_to_phenotypes.keys()):\n",
    "        for pheno in section_to_phenotypes[section]:\n",
    "            preds[pheno] = np.nanmean([candidate_labels[pheno][candidate_studies[c_ids]]\n",
    "                                   for c_ids in top_candidate_ids[s_dx]\n",
    "                                      if candidate_studies[c_ids] in candidate_labels[pheno]])\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3760c1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Left Ventricle: Normal left ventricular size by linear cavity dimension. Normal left ventricular size by volume Mild left ventricular hypertrophy. Normal left ventricular systolic function. LV Ejection Fraction is 60.0 %. Mild diastolic dysfunction. There is reversal of the E to A ratio and/or prolonged deceleration time consistent with impaired left ventricular relaxation. Doppler parameters and/or lateral mitral annular (E`) velocities are consistent with normal left ventricular filling pressures. [SEP]Resting Segmental Wall Motion Analysis: Total wall motion score is 1.0. There are no regional wall motion abnormalities [SEP]Right Ventricle: Normal right ventricular size. Normal right ventricular systolic function. [SEP]Left Atrium: The left atrium is normal in size. [SEP]Right Atrium: The right atrium is normal in size. [SEP]Atrial Septum: The interatrial septum is normal in appearance. [SEP]Mitral Valve: The mitral valve demonstrates normal function with trace physiologic regurgitation. The mitral valve demonstrates normal leaflet morphology. [SEP]Aortic Valve: The peak transaortic gradient is 5.0 mmHg The mean transaortic gradient is 3.0 mmHg No significant aortic stenosis or insufficiency. Trileaflet aortic valve. The aortic cusps appear mildly thickened. [SEP]Tricuspid Valve: Normal appearance of the tricuspid valve. Est RV/RA pressure gradient is 13.0 mmHg. Estimated peak RVSP is 16.0 mmHg. There is trivial tricuspid regurgitation. [SEP]Pulmonic Valve: Normal pulmonic valve function with trace physiologic regurgitation. Normal pulmonic valve appearance. [SEP]Pericardium: Normal pericardium with no pericardial effusion. [SEP]Aorta: The aortic root is normal in size. Sinus of Valsalva: 3.3 cm. The ascending aorta is normal in size. Ascending Aorta 3.5 cm. [SEP]IVC: The inferior vena cava is of normal size. The IVC diameter is 18.0 mm The inferior vena cava shows a normal respiratory collapse consistent with normal right atrial pressure (3 mmHg). [SEP]Pulmonary Artery: Estimated PA Pressure is 16.0 mmHg. PA systolic pressure is normal. [SEP]Pulmonary Veins: Pulmonary veins are normal in appearance and pulse Doppler interrogation shows normal systolic predominant flow. [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_report(encoded_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad92ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature logits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'impella': 0.0,\n",
       " 'ejection_fraction': 56.28,\n",
       " 'pacemaker': 0.02,\n",
       " 'rv_systolic_function_depressed': 0.0,\n",
       " 'right_ventricle_dilation': 0.0,\n",
       " 'left_atrium_dilation': 0.0,\n",
       " 'right_atrium_dilation': 0.0,\n",
       " 'mitraclip': 0.0,\n",
       " 'mitral_annular_calcification': 0.0,\n",
       " 'mitral_stenosis': 0.0,\n",
       " 'mitral_regurgitation': 0.0,\n",
       " 'tavr': 0.0,\n",
       " 'bicuspid_aov_morphology': 0.0,\n",
       " 'aortic_stenosis': 0.0,\n",
       " 'aortic_regurgitation': 0.0,\n",
       " 'tricuspid_stenosis': 0.0,\n",
       " 'tricuspid_valve_regurgitation': 0.0,\n",
       " 'pericardial_effusion': 0.0,\n",
       " 'aortic_root_dilation': 0.0,\n",
       " 'dilated_ivc': 0.02,\n",
       " 'pulmonary_artery_pressure_continuous': 22.08823529411765}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Feature logits\")\n",
    "predict_metrics(encoded_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83079d",
   "metadata": {},
   "source": [
    "It's good to pick ROC thresholds based on a val set from your institution. \n",
    "\n",
    "We provide ROC thresholds that maximize true positive rate and minize false positive rate on val studies from Cedars Sinai Medical Center in roc_thresholds.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1413c9-01b0-4fe0-8a60-21dafa959a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db844e-748c-43ea-b0d9-d2f41b46e0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26639f0d-7831-45a3-855b-813db955caf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
