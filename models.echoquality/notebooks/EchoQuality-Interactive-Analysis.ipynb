{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EchoQuality Interactive Visual Analysis\n",
    "\n",
    "This notebook provides interactive analysis and visualization of EchoQuality inference results.\n",
    "\n",
    "## Features:\n",
    "- Interactive score distribution analysis\n",
    "- Folder-by-folder performance comparison\n",
    "- Quality assessment breakdown\n",
    "- Error analysis and debugging\n",
    "- Custom filtering and exploration\n",
    "\n",
    "## Requirements:\n",
    "Make sure you have run inference first:\n",
    "```bash\n",
    "make inference\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summary data\n",
    "results_dir = '../results/inference_output'\n",
    "summary_path = os.path.join(results_dir, 'summary.json')\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    print(f\"‚ùå Summary file not found: {summary_path}\")\n",
    "    print(\"Please run 'make inference' first to generate results.\")\n",
    "else:\n",
    "    with open(summary_path, 'r') as f:\n",
    "        summary_data = json.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"üìä Found {summary_data['total_folders']} folders with {summary_data['total_files']} total files\")\n",
    "    print(f\"üìà Overall pass rate: {summary_data['overall_pass_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive DataFrame for analysis\n",
    "all_results = []\n",
    "folder_stats = []\n",
    "\n",
    "for folder_result in summary_data['folder_results']:\n",
    "    folder_name = folder_result['folder']\n",
    "    \n",
    "    # Collect folder statistics\n",
    "    if folder_result['num_processed'] > 0:\n",
    "        folder_stats.append({\n",
    "            'folder': folder_name,\n",
    "            'pass_rate': folder_result['pass_rate'],\n",
    "            'num_files': folder_result['num_files'],\n",
    "            'num_processed': folder_result['num_processed'],\n",
    "            'pass_count': folder_result['pass_count'],\n",
    "            'fail_count': folder_result['fail_count']\n",
    "        })\n",
    "    \n",
    "    # Collect individual file results\n",
    "    if 'results' in folder_result:\n",
    "        for file_id, result in folder_result['results'].items():\n",
    "            all_results.append({\n",
    "                'folder': folder_name,\n",
    "                'file_id': file_id,\n",
    "                'score': result['score'],\n",
    "                'status': result['status'],\n",
    "                'assessment': result['assessment'],\n",
    "                'path': result['path']\n",
    "            })\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_folders = pd.DataFrame(folder_stats)\n",
    "\n",
    "print(f\"üìã Created DataFrames:\")\n",
    "print(f\"   - Individual results: {len(df_results)} records\")\n",
    "print(f\"   - Folder statistics: {len(df_folders)} folders\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nüìä Quick Statistics:\")\n",
    "print(f\"   - Mean score: {df_results['score'].mean():.4f}\")\n",
    "print(f\"   - Median score: {df_results['score'].median():.4f}\")\n",
    "print(f\"   - Score std: {df_results['score'].std():.4f}\")\n",
    "print(f\"   - Pass rate: {(df_results['status'] == 'PASS').mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interactive Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics table\n",
    "summary_stats = pd.DataFrame([\n",
    "    ['Total Folders', summary_data['total_folders']],\n",
    "    ['Total Files', summary_data['total_files']],\n",
    "    ['Processed Files', summary_data['total_processed']],\n",
    "    ['Pass Count', summary_data['total_pass']],\n",
    "    ['Fail Count', summary_data['total_fail']],\n",
    "    ['Overall Pass Rate', f\"{summary_data['overall_pass_rate']:.2f}%\"]\n",
    "], columns=['Metric', 'Value'])\n",
    "\n",
    "display(HTML(\"<h3>üìä Overall Statistics</h3>\"))\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive score distribution\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add histogram\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df_results['score'],\n",
    "    nbinsx=50,\n",
    "    name='Score Distribution',\n",
    "    opacity=0.7,\n",
    "    marker_color='skyblue'\n",
    "))\n",
    "\n",
    "# Add threshold line\n",
    "fig.add_vline(x=0.5, line_dash=\"dash\", line_color=\"red\", \n",
    "             annotation_text=\"Pass/Fail Threshold (0.5)\")\n",
    "\n",
    "# Add mean and median lines\n",
    "mean_score = df_results['score'].mean()\n",
    "median_score = df_results['score'].median()\n",
    "\n",
    "fig.add_vline(x=mean_score, line_dash=\"dot\", line_color=\"green\", \n",
    "             annotation_text=f\"Mean: {mean_score:.3f}\")\n",
    "fig.add_vline(x=median_score, line_dash=\"dot\", line_color=\"orange\", \n",
    "             annotation_text=f\"Median: {median_score:.3f}\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Interactive Quality Score Distribution',\n",
    "    xaxis_title='Quality Score',\n",
    "    yaxis_title='Frequency',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Folder Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive folder performance scatter plot\n",
    "fig = px.scatter(df_folders, \n",
    "                x='num_processed', \n",
    "                y='pass_rate',\n",
    "                size='num_files',\n",
    "                hover_data=['folder', 'pass_count', 'fail_count'],\n",
    "                title='Folder Performance: Pass Rate vs Number of Processed Files',\n",
    "                labels={\n",
    "                    'num_processed': 'Number of Processed Files',\n",
    "                    'pass_rate': 'Pass Rate (%)',\n",
    "                    'num_files': 'Total Files'\n",
    "                })\n",
    "\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top and bottom performing folders\n",
    "top_folders = df_folders.nlargest(10, 'pass_rate')\n",
    "bottom_folders = df_folders.nsmallest(10, 'pass_rate')\n",
    "\n",
    "display(HTML(\"<h3>üèÜ Top 10 Performing Folders</h3>\"))\n",
    "display(top_folders[['folder', 'pass_rate', 'num_processed', 'pass_count', 'fail_count']])\n",
    "\n",
    "display(HTML(\"<h3>‚ö†Ô∏è Bottom 10 Performing Folders</h3>\"))\n",
    "display(bottom_folders[['folder', 'pass_rate', 'num_processed', 'pass_count', 'fail_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Assessment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment distribution\n",
    "assessment_counts = df_results['assessment'].value_counts()\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=assessment_counts.index,\n",
    "    values=assessment_counts.values,\n",
    "    hole=0.3\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Quality Assessment Distribution\",\n",
    "    width=600,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Display assessment statistics\n",
    "assessment_stats = pd.DataFrame({\n",
    "    'Assessment': assessment_counts.index,\n",
    "    'Count': assessment_counts.values,\n",
    "    'Percentage': (assessment_counts.values / len(df_results) * 100).round(2)\n",
    "})\n",
    "\n",
    "display(HTML(\"<h3>üìã Assessment Breakdown</h3>\"))\n",
    "display(assessment_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Filtering and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widgets for filtering\n",
    "score_range = widgets.FloatRangeSlider(\n",
    "    value=[0.0, 1.0],\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='Score Range:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f'\n",
    ")\n",
    "\n",
    "status_filter = widgets.SelectMultiple(\n",
    "    options=['PASS', 'FAIL'],\n",
    "    value=['PASS', 'FAIL'],\n",
    "    description='Status:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "folder_filter = widgets.SelectMultiple(\n",
    "    options=sorted(df_results['folder'].unique()),\n",
    "    value=sorted(df_results['folder'].unique())[:5],  # Default to first 5 folders\n",
    "    description='Folders:',\n",
    "    disabled=False,\n",
    "    rows=10\n",
    ")\n",
    "\n",
    "def update_analysis(score_range_val, status_val, folder_val):\n",
    "    # Filter data based on selections\n",
    "    filtered_df = df_results[\n",
    "        (df_results['score'] >= score_range_val[0]) &\n",
    "        (df_results['score'] <= score_range_val[1]) &\n",
    "        (df_results['status'].isin(status_val)) &\n",
    "        (df_results['folder'].isin(folder_val))\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìä Filtered Results: {len(filtered_df)} files\")\n",
    "    print(f\"üìà Pass Rate: {(filtered_df['status'] == 'PASS').mean() * 100:.2f}%\")\n",
    "    print(f\"üìâ Mean Score: {filtered_df['score'].mean():.4f}\")\n",
    "    \n",
    "    # Create filtered visualization\n",
    "    fig = px.histogram(filtered_df, x='score', color='status',\n",
    "                      title=f'Filtered Score Distribution ({len(filtered_df)} files)',\n",
    "                      nbins=30)\n",
    "    fig.show()\n",
    "    \n",
    "    # Show sample of filtered data\n",
    "    if len(filtered_df) > 0:\n",
    "        display(HTML(\"<h4>üìã Sample of Filtered Data</h4>\"))\n",
    "        display(filtered_df.head(10))\n",
    "    else:\n",
    "        print(\"‚ùå No data matches the current filters\")\n",
    "\n",
    "# Create interactive widget\n",
    "interactive_widget = widgets.interactive(\n",
    "    update_analysis,\n",
    "    score_range_val=score_range,\n",
    "    status_val=status_filter,\n",
    "    folder_val=folder_filter\n",
    ")\n",
    "\n",
    "display(HTML(\"<h3>üîç Interactive Data Explorer</h3>\"))\n",
    "display(interactive_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score Analysis by Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of scores by folder (top 15 folders by file count)\n",
    "top_folders_by_count = df_folders.nlargest(15, 'num_processed')['folder'].tolist()\n",
    "filtered_for_boxplot = df_results[df_results['folder'].isin(top_folders_by_count)]\n",
    "\n",
    "fig = px.box(filtered_for_boxplot, x='folder', y='score', color='status',\n",
    "            title='Score Distribution by Folder (Top 15 by File Count)')\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_layout(width=1000, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors from folder results\n",
    "error_analysis = []\n",
    "\n",
    "for folder_result in summary_data['folder_results']:\n",
    "    if 'error_stats' in folder_result:\n",
    "        folder_name = folder_result['folder']\n",
    "        error_stats = folder_result['error_stats']\n",
    "        \n",
    "        for error_type, count in error_stats['error_counts'].items():\n",
    "            if count > 0:\n",
    "                error_analysis.append({\n",
    "                    'folder': folder_name,\n",
    "                    'error_type': error_type,\n",
    "                    'count': count\n",
    "                })\n",
    "\n",
    "if error_analysis:\n",
    "    df_errors = pd.DataFrame(error_analysis)\n",
    "    \n",
    "    # Error type distribution\n",
    "    error_summary = df_errors.groupby('error_type')['count'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    fig = px.bar(x=error_summary.index, y=error_summary.values,\n",
    "                title='Error Type Distribution Across All Folders')\n",
    "    fig.update_layout(xaxis_title='Error Type', yaxis_title='Count')\n",
    "    fig.show()\n",
    "    \n",
    "    display(HTML(\"<h3>‚ö†Ô∏è Error Summary</h3>\"))\n",
    "    error_summary_df = pd.DataFrame({\n",
    "        'Error Type': error_summary.index,\n",
    "        'Total Count': error_summary.values,\n",
    "        'Percentage': (error_summary.values / error_summary.sum() * 100).round(2)\n",
    "    })\n",
    "    display(error_summary_df)\n",
    "else:\n",
    "    print(\"‚úÖ No errors found in the analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_folder(folder_name):\n",
    "    \"\"\"Detailed analysis of a specific folder\"\"\"\n",
    "    folder_data = df_results[df_results['folder'] == folder_name]\n",
    "    \n",
    "    if len(folder_data) == 0:\n",
    "        print(f\"‚ùå No data found for folder: {folder_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÅ Analysis for folder: {folder_name}\")\n",
    "    print(f\"üìä Total files: {len(folder_data)}\")\n",
    "    print(f\"üìà Pass rate: {(folder_data['status'] == 'PASS').mean() * 100:.2f}%\")\n",
    "    print(f\"üìâ Mean score: {folder_data['score'].mean():.4f}\")\n",
    "    print(f\"üìä Score range: {folder_data['score'].min():.4f} - {folder_data['score'].max():.4f}\")\n",
    "    \n",
    "    # Score distribution for this folder\n",
    "    fig = px.histogram(folder_data, x='score', color='status',\n",
    "                      title=f'Score Distribution for {folder_name}',\n",
    "                      nbins=20)\n",
    "    fig.show()\n",
    "    \n",
    "    # Assessment breakdown\n",
    "    assessment_counts = folder_data['assessment'].value_counts()\n",
    "    print(f\"\\nüìã Assessment Breakdown:\")\n",
    "    for assessment, count in assessment_counts.items():\n",
    "        percentage = (count / len(folder_data)) * 100\n",
    "        print(f\"   {assessment}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Example usage widgets\n",
    "folder_selector = widgets.Dropdown(\n",
    "    options=sorted(df_results['folder'].unique()),\n",
    "    description='Select Folder:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "analyze_button = widgets.Button(description=\"Analyze Folder\")\n",
    "\n",
    "def on_analyze_click(b):\n",
    "    analyze_folder(folder_selector.value)\n",
    "\n",
    "analyze_button.on_click(on_analyze_click)\n",
    "\n",
    "display(HTML(\"<h3>üîç Single Folder Analysis</h3>\"))\n",
    "display(widgets.HBox([folder_selector, analyze_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for further analysis\n",
    "output_dir = '../analysis_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save DataFrames\n",
    "df_results.to_csv(os.path.join(output_dir, 'all_results.csv'), index=False)\n",
    "df_folders.to_csv(os.path.join(output_dir, 'folder_statistics.csv'), index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats_dict = {\n",
    "    'total_files_analyzed': len(df_results),\n",
    "    'total_folders_analyzed': len(df_folders),\n",
    "    'overall_pass_rate': (df_results['status'] == 'PASS').mean() * 100,\n",
    "    'mean_score': df_results['score'].mean(),\n",
    "    'median_score': df_results['score'].median(),\n",
    "    'score_std': df_results['score'].std(),\n",
    "    'min_score': df_results['score'].min(),\n",
    "    'max_score': df_results['score'].max()\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'analysis_summary.json'), 'w') as f:\n",
    "    json.dump(summary_stats_dict, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Analysis results saved to: {output_dir}\")\n",
    "print(f\"üìÅ Files created:\")\n",
    "print(f\"   - all_results.csv: Individual file results\")\n",
    "print(f\"   - folder_statistics.csv: Folder-level statistics\")\n",
    "print(f\"   - analysis_summary.json: Summary statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Static Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save static plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('EchoQuality Analysis Summary', fontsize=16)\n",
    "\n",
    "# 1. Score distribution\n",
    "axes[0, 0].hist(df_results['score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(0.5, color='red', linestyle='--', label='Threshold')\n",
    "axes[0, 0].axvline(df_results['score'].mean(), color='green', linestyle='--', label='Mean')\n",
    "axes[0, 0].set_title('Score Distribution')\n",
    "axes[0, 0].set_xlabel('Quality Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Pass/Fail by status\n",
    "status_counts = df_results['status'].value_counts()\n",
    "axes[0, 1].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 1].set_title('Pass/Fail Distribution')\n",
    "\n",
    "# 3. Top folders by pass rate\n",
    "top_10_folders = df_folders.nlargest(10, 'pass_rate')\n",
    "y_pos = range(len(top_10_folders))\n",
    "axes[1, 0].barh(y_pos, top_10_folders['pass_rate'], color='lightgreen', alpha=0.7)\n",
    "axes[1, 0].set_yticks(y_pos)\n",
    "axes[1, 0].set_yticklabels([f[:20] + '...' if len(f) > 20 else f for f in top_10_folders['folder']], fontsize=8)\n",
    "axes[1, 0].set_title('Top 10 Folders by Pass Rate')\n",
    "axes[1, 0].set_xlabel('Pass Rate (%)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Score vs file count scatter\n",
    "axes[1, 1].scatter(df_folders['num_processed'], df_folders['pass_rate'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Number of Processed Files')\n",
    "axes[1, 1].set_ylabel('Pass Rate (%)')\n",
    "axes[1, 1].set_title('Pass Rate vs File Count')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'static_summary.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Static visualization saved to: {os.path.join(output_dir, 'static_summary.png')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
